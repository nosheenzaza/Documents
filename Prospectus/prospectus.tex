\documentclass[]{usiinfprospectus}
\usepackage[all]{xy}
\input{defs}

\usepackage{epigraph}
% \usepackage[T1]{fontenc}
% \usepackage{lmodern}
% \usepackage{lmodern}
\captionsetup{labelfont={bf}}

\author{Nosheen Zaza}


\lstset{language=scala}
\lstset{mathescape=false}

\title{Declarative Data-Centric Consistency }
% \subtitle{The (optional) sub title}
\versiondate{\today}

\begin{committee}
%With more than 1 advisor an error is raised...: only 1 advisor is allowed!
\researchadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Nate}{Nystrom}
\academicadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Nate}{Nystrom}
\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Committee}{Member1}
\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Committee}{Member2}
%\committeemember[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Committee}{Member3}
% \coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor1}
% \coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor2}
%\coadvisor[Universit\`a della Svizzera Italiana, Switzerland]{Prof.}{Research}{Co-Advisor3}
\phddirector{Prof.}{Stefan}{Wolf}
\end{committee}

\input{abstract}

\begin{document}
\maketitle


%!TEX root = prospectus.tex
\section{Introduction and Problem Domain}
% \epigraph{A meal prepared by two cooks is either salty or saltless}{Old Kurdish proverb}

% I think it is better to reorder like this: data is often replicated even under the hood, then we talk about sequential consistency and its problems, then other consistency models, then very weak consistency, then our proposal.

% Ideal consistency model, why not employed, before it was the case that it was only used under the hood, very few application level usages of that, but internet has brought many, because time is really critical, the yahoo example

% Define consistency of replicated data
% How it is usually not tolerated to observe different versions of the data by different clients
% Processors provide weak consistency and on top of that we build strongly consistent applications
% concurrent skip list map, fortran thread scheduling
% But the cloud has many applications
% managing these requires being even more careful
% then my old talk I guess. 
% I need to think about connecting program order to this, and also semantics of the data abstraction as a whole.

In an ideal concurrent environment, it is reasonable for programmers to expect that the latest update issued to a shared data abstraction will be the one visible to the next read issued by any client. However, a system will almost always include multiple copies of the same data abstraction through caching, buffering and replication, which might lead to observing multiple versions of data at the same time. Furthermore, updates are often reordered, either to allow optimizations, which is the case in many CPU architectures such as ARM~\cite{Chong:2008:RAW:1353522.1353528}, or because of communication medium delays, familiar to network application developers. Consequently, defining what `latest update' means is no longer straight forward. Programming in such environments requires defining a consistency model~\cite{adve1996shared} which serves as a contract between the data abstraction and its clients. The sequential consistency model~\cite{lamport1979make} fulfills the above mentioned expectation, thus providing serial semantics to concurrent programs. This model is expensive in terms of time, because it disallows many execution interleavings through some form of synchronization, ultimately forcing clients to wait. It also forbids some reordering-based optimizations. In reality most frameworks provide more relaxed consistency models, such as linearizability~\cite{herlihy1990linearizability}, causal consistency~\cite{raynal1995causal}, eventual consistency~\cite{terry1995managing}...etc 

With a few exceptions of applications with probabilistic nature such as~\cite{concurrentskiplistmap}, multicore applications try to exhibit a strongly-consistent behavior, and relaxed consistency provided by underlying frameworks has to remain in the hiding. However, exploiting relaxed consistency can allow for more concurrency. 
The internet has brought many more use-cases where application-level consistency can be relaxed with little compromise. As a motivating example, consider an online shopping application that replicates product data. When a client requests a list of all products that have 5-star ratings, it is not necessary to query all replicas in order to get the most up-to-date rating data. In fact, requiring acknowledgments from all replicas would cause unnecessary system load for a task that is inherently subjective. Real-world use-cases include metrics collection and product recommendations at eBay~\cite{cassandraebay}, and inbox search at facebook~\cite{lakshman2010cassandra}. Both employ Cassandra~\cite{lakshman2010cassandra}, an eventually-consistent~\cite{vogels2009eventually} storage backend.


% In certain applications, time is so expensive to the point where consistency and physical money are sacrificed instead. Amazon's inventory system is weakly consistent, and might sell items that are no longer available. In such cases, the order is cancelled and the client is offered a credit of 10\% towards future purchase[ref]. This is to ensure faster response and more availability. While this example might seem extreme, relaxed consistency models



% Propagating updates takes time, data stores may fail or operate slowly due to system load, and concurrent updates might not arrive in the same order they were issued. Traditionally, we need to spend more time to pay the cost of consistency, by making clients wait until the system reaches a strongly consistent state.


% The internet itself brought systems such as cassandra, which was built with distribution and replication built in, with radical desicions, no joins, no transactions, and this brought a new model of computing.


Relaxed consistency is rarely sufficient on its own for the requirements of a complete application. Typically, components requiring different consistency policies co-exist and operate. Great care must be taken to ensure correct program semantics in this case. Continuing with the previous example, consider  when the client adds an item from the list of items with 5-star ratings to his shopping cart. Because the list was not constructed by querying all replicas, it is possible that it includes some items that are no longer in stock. If the programmer forgets to query all replicas to ensure that the selected item is in stock prior to checkout. The client will end up buying a non-existing item.

% The problem of maintaining data consistency is not exclusive to distributed systems with explicit replication and application-level weak consistency. It is well known in concurrent programming literature. On the lowest level, data is replicated under the hood among main memory, caches and registers, and many common CPU architectures, such ARM-based processors, used in many handheld devices provide only a weak memory consistency model, on which applications with typically strong consistency requirements are built. Yet maintaining data consistency does not happen under the hood as well, on the contrary it is the heavy responsibility of the programmer, who must ensure that:
%
% \begin{enumerate}
% 	\item The program does not violate consistency.
% 	\item The compiler will not reorder the program in a way that violates consistency.
% 	\item The CPU will not reorder the program in a way that violates consistency.
% \end{enumerate}
%

Ongoing research on concurrency, programming languages and computer systems has produced many methodologies and tools that facilitate consistency management, and we are investigating how to combine and extend existing ideas in order to construct a framework that allows programmers to safely employ both strong and relaxed consistency in their applications.
% many of which can be adapted to a distributed environment with arguably more potential of success, because replication and consistency requirements are explicit and on application-level. Furthermore, the overhead of some techniques is better tolerated in a distributed environment.
The core idea we are studying is a declarative, data-centric approach to consistency management, originally proposed by Vaziri el al.~\cite{Vaziri:2006:ASC:1111320.1111067}. We believe that it has the potential to be adapted and enriched to provide a powerful framework for programmers to declaratively express not only atomicity as originally proposed, but also various consistency constraints. Based on these constraints, it is possible to delegate many parts of consistency management logic to the compiler and runtime. In this research prospectus, I start by describing the core ideas of this approach, then show how it can be adapted to support multiple consistency levels to naturally express common distributed computing use cases. Then I will list related work on consistency constraints and program properties that we believe are useful for programmers to express  declaratively. After that I will overview the research directions and challenges, and how we plan to conduct our work.

\section{Data-Centric Constraints}

When programming in a shared-memory environment, programmers must ensure that concurrent accesses to the same memory location do not lead to data races and inconsistent results. This is traditionally achieved by ensuring that instructions accessing shared data acquire locks, or are within a transaction. Such approaches have an operational, control centric flavor. The problem is that data races may occur if the programmer forgets synchronize even a single control flow path.

\subsection{Atomic Sets}
Vaziri et al.~\cite{Vaziri:2006:ASC:1111320.1111067} outlined a solution in 2006, that associates synchronization constraints with data, and based on these constraints, the compiler generates synchronization code for all operations that access this data. Figure 1(counter example from the paper) shows an example taken from that paper written in Java. It involves incrementing a counter concurrently. An atomic set is declared, which is a memory region to which all accesses must be synchronized, then class fields that need to be protected are added to this set. The compiler then synchronizes all public class methods that interact with the atomic set. The system also allows atomic sets that span multiple objects, as well as other features. 

\subsection{Adding Replicated Data Consistency Constraints}

In many replicated key-value storage systems, such as Cassandra~\cite{lakshman2010cassandra}, Dynamo~\cite{hastorun2007dynamo}, and Riak~\cite{redmond2012seven}, clients can tune consistency by specifying the number of replicas on which an operation must succeed before returning an acknowledgment to the client application. In Cassandra, consistency is configured on the level of cluster, data center, or individual I/O operations. Going back to our online shopping example, instead of relying solely on the programmer to choose the correct consistency level every time he writes code that interacts with the contents of the shopping cart, we can attach a policy to shopping cart data region, stating that all operations must have the consistency level ALL, meaning all replicas must acknowledge the successful completion of the operation. We gain two benefits following this data-centric approach: it is now possible to infer the correct consistency level for all operations dealing with the shopping cart data, and if the programmer specifies a lower consistency level, such as ONE, which only requires an acknowledgment from one replica, it is possible now to show an error/warning stating that this is probably not what is meant to be done. Furthermore, Since the list or 5-star products was read with consistency level ONE, it represents a memory region that is not strongly consistent. It will not be possible to assign an item form this region to the shopping cart, as it represents a forbidden information flow path. 
\section{Research Prospectives}

Inspired by the above mentioned work, we aim to present a programming environment that enables developing distributed applications with better data abstractions. We have identified the following aspects that we need to investigate: 

\begin{itemize}
	\item Defining data region abstractions.
	\item Identifying data and operation properties and constraints that would be useful to infer/express.
	\item Techniques and tools that enable programmers to express desired properties with minimum overhead.
\end{itemize}	

Our aim is to create a development framework that enables programmers to code in the presence of asynchronous operations and data replication in more intuitive ways than currently offered. 

In the following sections, we describe each research problem, and show related work.

\subsection{Data Region Abstractions}
We need to determine which data region abstraction(s) to employ, because choosing the right abstractions eases defining constraints on data. Many data abstraction in various systems come with baked-in consistency constraints. Having good default definitions of such constraints often relieves the programmer from having to define them himself. We also believe that having replication defined as an explicit property of data makes reasoning about it easier for the programmer.The following list contains various data abstractions and built-in consistency guarantees. 

\begin{enumerate}
	\item The Jave memory model~\cite{javamemorymodel} ensures atomic semantics for one-word primitive types, such as \texttt{int}. \texttt{long} and \texttt{double} are not required to be atomic by the language specification.  
	\item In many database systems, row updates are atomic. For instance, In MySQL~\cite{mysql}, the MYISAM storage engine locks tables when an update is issued, while InnoDB engine locks the row being updated. In Cassandra, rows are atomic units. 
	\item Objects in object oriented languages span over multiple memory locations, and group together data and operations~\cite{rentsch1982object}. Objects can be arbitrarily nested and passed around to form complex graphs, and accesses to object data are typically unordered and synchronous. Objects present a difficult case for constraint attachment, because many threads can have references to different parts of an object and update them concurrently, possibly making temporary invariant violation visible among threads. Even if each object in the graph is properly synchronized, the collection of objects might still have incorrect semantics~\cite{artho2003high}.  Furthermore, it is hard to reason about nesting of objects bearing conflicting constraints. There is a large body of ongoing research on ownership types and alias management that aim to restrict arbitrary referencing of objects~\cite{muller1999universes, clarke2008minimal, haller2010capabilities}.
	\item In the current implementation of atomic sets~\cite{dolby2012data}, Vaziri el al. employ a simple ownership type system that ensures objects are only accessed via their owners. Without this constraint, it would be hard to perform static analysis. Atomic sets presented in this work are static data containers that may span the data of multiple objects. data within an atomic set share the same lock. However atomic sets only express ``shallow-locking'', if nested objects must be locked, they need to be explicitly added to the containing object's atomic set. It is also not possible to dynamically grow or shrink atomic sets. 
	\item X10~\cite{charles2005x10} supports places~\cite{x10spec}, which are  a distribution-friendly data abstraction that encapsulate the binding of activities and globally addressable memory. An activity may synchronously access data items only at the place in which it is running. It may atomically update one or more data items, but only in the current place. If it becomes necessary to read or modify an object at some other place, a place-shifting operation can be used. This approach ensures that expensive operations (e.g., those which require communication) are readily visible in the code. The implicit constraint in this system is the locality of data. 
	\item DPJ~\cite{bocchino2009parallel} regions are hierarchal heap partitions used to disambiguate accesses to distinct objects, as well as distinct parts of the same object. A type-checker then ensures that there are no conflicting accesses to overlapping memory regions between concurrent tasks. 
	\item The actor model~\cite{agha1985actors} differs in two fundamental ways from the object model: all accesses to actor state are serial and asynchronous. This model is a better fit for a networked environment, and is gaining popularity with languages such as Erlang~\cite{armstrong1993concurrent} and Scala~\cite{odersky2004overview}  employing it successfully. Typically, actors must not expose their internal data by any means other than message passing. In practice, systems such as Akka~\cite{akka} do not enforce that, and rely on the programmer to maintain actor encapsulation. Because of their properties, actors are easily distributable, however it is hard to introduce concurrency within an actor. Habanero Java and Habanero Scala~\cite{imam2012integrating} enable controlled concurrency within an actor. 
	\item Coqa~\cite{liu2008coqa} is a programming language that supports different kinds of objects with properties similar to traditional objects or actors.
\end{enumerate}

% As there are wrappers to data, there are wrappers to operations. A database transaction, or a method body are two commonly used wrappers. Investigating such wrappers is an important aspect to our work, because we need to attach constraints to operations as well, and sometimes it is more convenient to infer the consistency of a memory region from the constraints of the operation interacting with it or from its properties.
%
% \begin{enumerate}
% 	\item In databases, isolation levels can be attached to transactions~\cite{bernstein2009sql}. The consistency of dataset resulting from a transaction would depend of the specified isolation level.
% 	\item A problem with object-oriented systems is that operations on data are grouped as functions, function bindings in many systems are determined at runtime. Functions can perform arbitrary reads and writes, and because they are dynamically bound, it is hard to tell at compile time which pieces of data are affected by a function. Effect systems allow the programmer to  declaratively attach effects to methods, and some of them can infer the effects partially~\cite{marino2009generic}. Cormac and Shaz propose an effect system that declares atomicity as a property of methods~\cite{flanagan2003type}.
% 	\item Software transactional memory~\cite{shavit1997software} borrows the ideas of database transactions and apply them to generic programs. Such systems are known for they high overhead because of the delay of creating operation logs and performing roll-backs. Not all software systems can tolerate such overheads. However transactions are easier to reason about than locking, and they are composable.
% 	%\item fork join vs async finish and rooted heirarchies, same to actor
% \end{enumerate}
	 
% In conclusion, if the coding environment is aware of operation effects, this can allow for more concurrency. This of course comes with the overhead of encoding the effects by the programmer.


\subsection{Consistency Constraints}
We now list some other constraints that enable better control of consistency by making some program properties explicit to the programmer or the development environment. 
\begin{enumerate}
\item Monotonicity: this property means that the final order or content of input will never cause any earlier output to be ``revoked'' once it has been generated. This means that any order of operation generates the same consistent result. Bloom is a distributed language that exploits this property by providing programmers with order-independent abstractions, which encourages a programming style that minimizes coordination requirements and guarantees consistency.~\cite{alvaro2011consistency}

\item Latency and failure: X10, Scala and other languages supports futures~\cite{hallerfutures}. These constructs materialize latency and failure as datatypes and operation effects. Their advantage is that they make these code properties explicit, In distributed environment, failure of operations should always be considered. Multicore programers need not be concerned with a CPU core going down or instructions getting lost between system buses, but network developers always face node failure and message loss. Futures make failure handling a first-class action in a distributed language.

\item Locality: Loci~\cite{wrigstad2009loci} is a pluggable type system that enables programmers to make thread locality of data explicit and statically checkable. This has many advantages: programmers need not to worry about the consistency of data proved to be local, and compilers can perform more optimizations with such data. 

% \item strong eventual consistency I will add that once I know what is 
% talks about!
\item Commutativity and Inverses: DPJ and the transactional boosting method~\cite{herlihy2008transactional} allow programmers to specify commutative and inverse methods. Commutativity permits re-ordering of method calls while ensuring deterministic semantics, and inverses allow creating more compact rollback logs for transactional memory. 
	\item Isolation Levels: In databases, isolation levels can be attached to transactions~\cite{bernstein2009sql}. The consistency of dataset resulting from a transaction would depend on the specified isolation level.
	\item Effects: A problem with object-oriented systems is that operations on data are grouped as functions. A function binding in many systems is determined at runtime. Functions can perform arbitrary reads and writes, and because they are dynamically bound, it is hard to tell at compile-time which pieces of data are affected by a function. Effect systems allow the programmer to  declaratively attach effects to methods, and some of them can infer the effects partially~\cite{marino2009generic}. Read/write effect systems exist~\cite{Lucassen:1988:PES:73560.73564}. Cormac and Shaz~\cite{flanagan2003type} propose an effect system that declares atomicity as a property of methods.
	\item Transactional Memory: Software transactional memory~\cite{shavit1997software} borrows the ideas of database transactions and apply them to generic programs. Such systems are known for they high overhead because of the delay of creating operation logs and performing roll-backs. Not all software systems can tolerate such overheads. However transactions are easier to reason about than locking, and they are composable.
	\item Other replicated-data consistency guarantees: There are many other consistency level between ONE and ALL mentioned earlier. Cassandra also support QURORUM, which employs a quorum-based technique~\cite{gifford1979weighted}. Session guarantees~\cite{terry1994session} provide an application with a view of the replicated database that is consistent with its own reads and writes performed in a session even though these operations may be directed at different servers. Guarantees are provided within the context of a session. a single application may create several sessions that it uses to exercise fine-grain control over the guarantees it desires. 

\end{enumerate}


\section{Research Directions}
Current development tools need to be extended in order to support programming patterns of distributed applications. An asynchronous model of computing is favored, and programming languages are starting to natively support this model. The declarative approach of programming now trending, and programming languages such as Scala facilitate it and encourage programmers to create declarative, domain specific languages tailored for the needs of a class of applications. We plan to build declarative programming language constructs that enable data-centric, global reasoning of various consistency policies, naturally support replication, asynchronous computing and materialize latency, failure and streaming. Because the object model is still very commonly used, we also plan to focus on implementing our model in an object-oriented environment. This will require us to build ownership and effect systems to tame this model and make it suitable for use under our language. We also plan to further investigate code properties and constraints, other than the ones we mentioned earlier to enrich our system and make it easier to use. 

\section{Achievements}
So far I have achieved the following:
\begin{itemize}
	\item Implemented atomic sets~\cite{dolby2012data} in Scala.
	\item Attended a workshop on software correctness and reliability at ETH Zurich.
	\item Attended the following courses: Bugs 2013 (4 ECTS) and Introduction to Ph.D. studies (2 ECTS) 
	\item Attended ECOOP 2014 with co-located workshops and summer schools.
	\item Served as a TA for the following courses: Programming Fundamentals 3 (4 ECTS) and Automata and Formal Languages (2 ECTS). 
	%\item Volunteered for the 10 year anniversary of USI
\end{itemize}

%Asynchronous operations then depending on acknoledgements it the way to go, supported by akka actors, and actors in erlang.

% Cases where information among replicas are not the same are less common that one might think, typically, it would take milliseconds to synchronize information at all replicas~\cite{cockcroft2011benchmarking}, however, with some systems processing millions of requests, rare events do occur, and must be considered when designing a system~\cite{vogels2009eventually}.
% Also, there are cases where we want to ensure not only coherence of information among all replicas, but we also need ordered access.


% programmers from making common programming mistakes. Java bakes in object locks, the infamous synchronized keyword, as well as volatile. Adding checked exceptions forces the programmer to handle them. Futures in Scala force also enforce handling error by default rather than leaving it as a forgettable option, and also manifest the fact that something will take time, and thus this should be handled. What else can we bake in to support better syntax and protection?

% A data consistency model is needed whenever multiple copies of data often replicate (note that replication is not the only source of consistency problems)~\cite{adve1996shared}, and access synchronization. Often, underlying systems do not provide strong consistency guarantees, and ensuring consistency is moved to the application layer, examples are CPU architectures[], or eventually consistent systems. Sometime we also want to allow relaxed consistency to improve performance and increase consistency, such in [skip lists], or many distributed applications [ref].

% This works ok when we want to have  serializability vs mess. But what if we have
% other consistency levels?

% There are many problems related to this as well. What are operations in an object-oriented language? What are data boundaries? How to compose effeciently?
%
% Also if we merge two sets, what kind of locks should we aquire for sets with different consistencies?

% Want to talk about wait free things and delegated isolation.

\bibliographystyle{abbrv}
\bibliography{../NosheenZazaBib}
\end{document}
