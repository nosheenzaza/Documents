%! TEX root = main.tex


\section{The  Language}
I need to describe it through my somewhat complicated example, then show the
possible orderings of operations based on the consistency levels. I will also
show legal and illegal consistency policy nestings. The latter examples, should
be done maybe after explaining the theory on consistency level combinations.
Other than that. 

\section{An Online Shopping Example}
This example introduces a data schema that is denormalized. Denormalization is a
common strategy in industry when designing distributed data storage. This is
because joining data from 


\subsection{Consistency Level Strength}
A consistency policy in effect, limits the numbers of orderings of atomic
operations on an atomic data abstraction. Hence if we use the notion of number
of allowed permutations of operations as a measure of the strength of
consistency (there may be as well many other ways to think about a measure of
consistency strength, but let's stick to this one at the moment), then we can
come up with the following definitions:

\subsubsection{Consistency level ordering according to strength}
A consistency level B is comparably weaker than a consistency level A, if it
allows more permutations of operations than B. Hence, there can be a total order
of consistency levels A and B, where B < A. The implication here is that we
cannot have a policy A on a compound data set, where there is a subset annotated
with consistency level B (invariants guaranteed by A cannot be enforced with B),
however, the policy of a compound set, that contains a subset named A can 

However, a consistency level B is non-comparably weaker than A is it allows
permutations that A prohibates. In this case, the same as above applies, but we
cannot have a total order on A and B, because both A < B and B < A! This means
that neither A nor B can be made policies on a superset of sets having a policy
B or A respectively. 

Note that strength of operations also depends on the observer, if we ensure
separation of local observers in respect to certain data sets, then it is ok to
enforce local orderings, the strength of these will of course be weaker than
global policies, from this it follows that local sets have weaker policies than
global ones.

The parameters then to comparing permutations then are the observer's view,
nature of operations (I think monotonic operations challenge my logic here), and
target of operations. I can solve the problem by assuming policies on common
structures and assuming no knowledge about operations. 

If I add only reads to my system, I can incorporate that into the information flow logic,
to optimize reads. 

We must not forget that, in order to make the compositional property hold and
make sense, the consistency policy in question must be composable, that is, make
sense in the eyes of a more global observer!

\subsection{How unitfor equivelant affects transaction orderings}
A serial transaction is a transaction that requires confirmation of all
operations, and that operations are applied 
in one atomic step. An eventual transaction required confirmation of all
operations, but does not require atomic visibility, if an eventual transaction
fails, all changes are rolled back, respecting the consistency of data sets it
operates on! 

e have a concept of an observer's (concretely here, transaction issued by a
client). I view a unitfor, or a consistency policy that spans more than one data
item as a set with respect to an operation with a larger view, once such a set
is defined, it becomes a global observer of the combined data items, the
implication of this, other than the need to serialize reads/writes to both data
items at once, is that no lower consistency transactions can anymore be issued
against the larger set (we do not care about the small, more blind transactions
I think (having doubts here, maybe depends whether they modify contents vs
context)), further more, all operations of transactions with the same
consistency is respect to a data item are serialized. 

After actually writing down the above, I think it is clear that consistency with
respect to an operation actually means setting a minumum consistency level for
the large set! which is enforced for other future operations to be defined. So
it is still data centric, and comes from the need of maintaining an invariant
for an operation. 

I think a transfer construct can be useful, as different rules apply to
trasferring from weak -> strong set or vice versa, concerning visibility of both
operations. 

Many operations are transfers and reflections of state, I think they deserve
their own constructs. 

\subsection{Transaction Semantics in the Language}
The definition of a transaction in this language differs from a common transaction in a
relational database. While a common transaction ensures atomicity, consistency, isolation.
durability and durability. The basic propoerty of a transaction in our system in
durability and consistency, atomicity and isolation of the transaction depend on
the data it manipulates, the consistency depends on the consistency policy of
the sets a transacton touches, and the isolation level again depends on the
consistency level of touched data. \\

Assume we have a basic transaction, operating on data with no consistnecy
annotations. The only guarantee of such transactions is that the success of the
transaction is not reported until all sub-operations have reported success. In
the case of a transaction that touches both strong and weak sets, the success
of the transaction is reported according to the sets it touches. The overall
consistency of such a transaction is that of the weaker set. 

Ramp Transaction (what was exactly the difference between that and snapshot
isolation?) guarantees that all subsequent reads or writes will observe that
different parts of a data item are consistent.

Strong Transaction ensures isolation and consistency, like a strict transaction. 
The runtime system orders reports transaction success and orders transactions depending on the desired consistency
level.

When a transaction fails to commit, rollback is not the only options, For
example, in the case of index and data, we can deffer creating the index instead
of the costly operation of rolling back the consistency data. In case of
shopping, if a checkout fails, we can r




