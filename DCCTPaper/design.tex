%! TEX root = main.tex


\section{Consistency Sets Design}

In this chapter, we describe the model of consistency sets, how they restrict
the interaction of transactions touching multiple sets, and the rules governing
combining and nesting consistency sets.

\subsection{Consistency Level Strength}
A consistency policy, in effect, limits the number of orderings of atomic
operations on an atomic data abstraction, which we refer to as a consistency
set. Hence if we use the notion of number
of allowed permutations of operations as a measure of the strength of
consistency, where the less permutations, the stronger the policy.
(there may be as well many other ways to think about a measure of
consistency strength, but let's stick to this one at the moment), then we can
state with the following definitions:

\subsubsection{Consistency level ordering according to strength}
A consistency level B is comparably weaker than a consistency level A, if it
allows more permutations of operations than B. Hence, there can be a total order
of consistency levels A and B, where B < A. The implication here is that we
cannot have a policy A on a compound data set, where there is a subset annotated
with consistency level B (invariants guaranteed by A cannot be enforced with B),
however, the policy of a compound set, that contains a subset named A can 

However, a consistency level B is non-comparably weaker than A is it allows
permutations that A prohibates. In this case, the same as above applies, but we
cannot have a total order on A and B, because both A < B and B < A! This means
that neither A nor B can be made policies on a superset of sets having a policy
B or A respectively. 

Note that strength of operations also depends on the observer, if we ensure
separation of local observers in respect to certain data sets, then it is ok to
enforce local orderings, the strength of these will of course be weaker than
global policies, from this it follows that local sets have weaker policies than
global ones.

The parameters then to comparing permutations then are the observer's view,
nature of operations (I think monotonic operations challenge my logic here), and
target of operations. I can solve the problem by assuming policies on common
structures and assuming no knowledge about operations. 

If I add only reads to my system, I can incorporate that into the information flow logic,
to optimize reads. 

We must not forget that, in order to make the compositional property hold and
make sense, the consistency policy in question must be composable, that is, make
sense in the eyes of a more global observer!

\subsection{How unitfor equivelant affects transaction orderings}
A serial transaction is a transaction that requires confirmation of all
operations, and that operations are applied 
in one atomic step. An eventual transaction required confirmation of all
operations, but does not require atomic visibility, if an eventual transaction
fails, all changes are rolled back, respecting the consistency of data sets it
operates on! 

e have a concept of an observer's (concretely here, transaction issued by a
client). I view a unitfor, or a consistency policy that spans more than one data
item as a set with respect to an operation with a larger view, once such a set
is defined, it becomes a global observer of the combined data items, the
implication of this, other than the need to serialize reads/writes to both data
items at once, is that no lower consistency transactions can anymore be issued
against the larger set (we do not care about the small, more blind transactions
I think (having doubts here, maybe depends whether they modify contents vs
context)), further more, all operations of transactions with the same
consistency is respect to a data item are serialized. 

After actually writing down the above, I think it is clear that consistency with
respect to an operation actually means setting a minumum consistency level for
the large set! which is enforced for other future operations to be defined. So
it is still data centric, and comes from the need of maintaining an invariant
for an operation.

The above reformulated:
Let's think about consistency polices as means of preserving invariants through 
limiting possible operation interleavings. Let's also claim that an Invariant violation 
is only detected by actually reading all data items affected by some update, this is 
intuitive to see because, we can agree that updates in a linearisable transaction 
updating many items will not happen instantaneously to all the items, and this is 
acceptable as long as observing all the items happens right after, or if writes that 
are based on a subset of values written in such a transaction happen after all the
updates of that subset. Once we introduce an operation, with an invariant that requires 
observing a data item, composed of multiple data items as one atomic item, then any other 
operation on that composed atomic item must not have a weaker consistency policy, or else 
we cannot guarantee the operation with a higher consistency, be it read or write will be
able to observe the invariant or maintain it. We can see that, what we did indirectly was 
to attach a new policy that is actually data centric, to the compound atomic data item 
touched by a transaction. This means that, whenever we have at least one operation of a 
compound data item with a certain consistency level, this consistency level represents 
the lowest allowed consistency of any other operations that operate of the scale of that 
compound data item, and hence it make sense to annotate the compound data item with that
consistency level. With the rules we have already defined about the consistency of 
operations on component data items, we can see that the system is not stricter that what 
is actually needed. \\

Note that, it is possible that a transaction commit protocol can actually commit
a transaction that preserves strong consistency without actually coordinating
with as many nodes as usually needed, assuming no other transaction depending on
the current one is running, we can actually run the transaction as if it
requires only eventual consistency (Eduardo's work). However note that this is
merely an optimization. In the language, the programmer still needs to annotate
the transactions in a way to preserve the semantics of the language. \\

A programmer may still want to annotate a transaction with a policy, if, for
instance, what is needed is reading the data quickly, without caring about the
consistency level. Such transactions will not be allowed to write back, or
anywhere that expects strongly consistent data.




I think a transfer construct can be useful, as different rules apply to
trasferring from weak -> strong set or vice versa, concerning visibility of both
operations. 

Many operations are transfers and reflections of state, I think they deserve
their own constructs.

\subsection{Read and Write Permissions}

\begin{itemize}

\item The consistency policies on a set concern writes only, however,
consistency of requested reads is restricted by the policies too.

\item A set that requires a consistency level x for writes, never accepts weaker or
stronger policies.

\item A set composed of subsets, cannot have a consistency level stronger than
the consistency of the weakest component set.

\item A transaction reading from a set with consistency level x cannot request
to read at a higher consistency level. It can, however, read at a weaker level.

\end{itemize}

\subsection{Transaction Semantics in the Language}
The definition of a transaction in this language differs from a common transaction in a
relational database. While a common transaction ensures atomicity, consistency, isolation.
durability and durability. The basic propoerty of a transaction in our system in
durability and consistency, atomicity and isolation of the transaction depend on
the data it manipulates, the consistency depends on the consistency policy of
the sets a transacton touches, and the isolation level again depends on the
consistency level of touched data. \\

Assume we have a basic transaction, operating on data with no consistnecy
annotations. The only guarantee of such transactions is that the success of the
transaction is not reported until all sub-operations have reported success. In
the case of a transaction that touches both strong and weak sets, the success
of the transaction is reported according to the sets it touches. The overall
consistency of such a transaction is that of the weaker set. 

Ramp Transaction (what was exactly the difference between that and snapshot
isolation?) guarantees that all subsequent reads or writes will observe that
different parts of a data item are consistent.

Strong Transaction ensures isolation and consistency, like a strict transaction. 
The runtime system orders reports transaction success and orders transactions depending on the desired consistency
level.

When a transaction fails to commit, rollback is not the only options, For
example, in the case of index and data, we can deffer creating the index instead
of the costly operation of rolling back the consistency data. In case of
shopping, if a checkout fails, we can..




