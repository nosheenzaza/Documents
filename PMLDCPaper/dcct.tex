\documentclass[numbers]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{listings}
\usepackage{txtt}
\usepackage{color}
\usepackage{defs}
\usepackage{hyperref}

\newcommand{\cL}{{\cal L}}

%TODO
% 1) find a way to handle nestings, check the work of atomic sets and loci again
% this was my problem because of assuming that nested levels of consistency in
% an operation results imply the same things when talking about nested data
% regions.
% 2) define all version combinations for a number of cosistency policies, make
% it clear what can and cannot be supported.
% so in theory everything can be defined in terms of current versions, and how
% next versions relate to each other in a set. 
% 3) find a way to combine this with the mixing theorem of Adya, 
% I state that my system does not assume transactions know what they are doing,
% and also makes it clear that we need to write at minimum isolation level
% declared on a set.
% TODO what is the state in database systems for atomicity of individual sql
% statements? Can we guarantee isolation level 3 if sql statements are not
% atomic?
% TODO describe handlind deletions and predicate operations over entities later,
% or mention it as future work.
% TODO should I have set policies for "relations"? or not as they do not exist
% in distributed stores?
% TODO  look again at sagas

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

%TODO fix copyright to what I received.
%TODO remove preprint
\conferenceinfo{PMLDC'16,}{July 17 2016, Rome, Italy}
\copyrightyear{2016}
\copyrightdata{978-1-4503-4775-4/16/7}
\copyrightdoi{2957319.2957377}


% Uncomment the publication rights you want to use.
\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Data-centric Consistency Policies} % 'preprint' option specified.

\title{Data-centric Consistency Policies}
\subtitle{A Programming Model for Distributed Applications with Tunable Consistency}

\authorinfo{Nosheen Zaza}
           {Universit\`a della Svizzera italiana (USI)}
           {nosheen.zaza@usi.ch}
\authorinfo{Nathaniel Nystrom}
           {Universit\`a della Svizzera italiana (USI)}
           {nate.nystrom@usi.ch}

\maketitle

% Abstract change: mention that there is support for for reasoning about 
% multiple consistency levels in research, however the frameworks are
% complicated, ours is good in that it is simpler. 
\begin{abstract}
The consistency level of operations over replicated data is 
an important parameter in distributed applications. It impacts correctness, 
performance, and availability. It is now common to find single applications
using many different consistency
levels at the same time; however, current commercial frameworks do not
provide high-level abstractions for specifying or reasoning about different
consistency properties of an application. Research frameworks that do tend to require a
substantial effort from developers to specify operation dependencies, orderings
and invariants to be preserved.
We propose an approach for specifying consistency properties based on the
observation that
correctness criteria and invariants are a property of data, not operations.
Hence, it is reasonable to define the consistency properties
required to enforce various data invariants on the data itself rather than on the operations. 
The result is a system that is simpler to describe and reason about. 
In this paper, we outline an abstract model of programming language constructs
and a static checker for data-centric consistency control, and demonstrate
this model through a simple prototype programming language implementation.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
distributed systems, eventual consistency, static analysis, domain-specific programming languages

\section{Introduction}
In modern distributed systems, data is often replicated 
to enhance availability and resilience to failure, and to improve performance by
placing replicas closer to clients. However, managing the consistency of 
replicated data is a nontrivial problem. Ideally, all clients should observe
the most recent data at all times.
Achieving this consistency level through
linearizability~\cite{herlihy1990linearizability} or
serializability~\cite{papadimitriou1979serializability} entails
well-documented costs in terms of latency, availability, and partition
tolerance~\cite{bailis2013highly, kraska2013mdcc}. For this reason,
many weaker consistency levels have been proposed,
such as eventual consistency~\cite{vogels2009eventually}, causal
consistency~\cite{lamport1978time, schiper1989new}, and snapshot isolation~\cite{kemme2000database}.
The choice of one of the above consistency levels depends on application
correctness and performance requirements. For instance, in a flight booking
application, there is no way eventual consistency alone
can guarantee that a single airplane seat will not be assigned to two different
people~\cite{bailis2014coordination, bailis2013highly}. However, eventual
consistency works well
in many non-critical cases, for example, creating and showing product
recommendations, and is widely used in practice~\cite{eventualeBay,eventualNetflix,hastorun2007dynamo}.

In current systems, to enforce a certain consistency
property in an application, a developer selects a storage system that
supports this property, then specifies consistency settings per
operation or per sequence of operations. However, mapping high-level 
application consistency requirements to a combination of
low-level consistency settings on operations is often tedious and error-prone.
For instance, Cassandra~\cite{lakshman2010cassandra} and Riak~\cite{riak} allow 
specifying one of many consistency settings for operations:
\texttt{one}, \texttt{two}, \texttt{quorum}, \texttt{all}, etc., describing
the set of nodes that need to acknowledge the success of an operation. It
is not immediately clear how consistency levels such as causal consistency or
linearizability can be achieved, if at all, using these systems. The Cassandra documentation
states that, to achieve ``strong consistency''\footnote{Strong consistency in
Cassandra implies
neither linearizability or serializability~\cite{sivaramakrishnan2016representation}.} clients must issue quorum reads and writes,
however, nothing prevents a misbehaving client from issuing concurrent \texttt{one} writes and
\texttt{quorum} reads on the same data, thus violating strong consistency for other clients. Another
problem is when a client issues \texttt{all} reads and writes, which is completely
unnecessary for enforcing any high-level consistency policies. Similar issues
occur in relational database systems, where it cannot be guaranteed that a
sequence of transactions running at different isolation levels preserves the intended
invariants~\cite{gray1992transaction}. 

% TODO the solution is not only for safety, but also for expressiveness. 
To tackle such problems, we propose a data-centric, declarative programming
model inspired by~\cite{dolby2012data}, where high-level consistency
requirements needed to enforce certain data integrity invariants 
are declared directly over data regions, and low-level 
consistency settings for operations interacting with
regions are assigned or restricted accordingly. We
describe the abstract model in Section~\ref{sec:model}, and in
Section~\ref{sec:example} we motivate its usefulness in addressing the above 
issues by showing an example in a prototype language implementing
it.\footnote{The language is a work in progress. The project that can be found at
  \url{https://github.com/amanjpro/languages-a-la-carte/}.}
We describe the mapping between our language's data-centric declarations and 
underlying operational consistency settings of the target storage system in 
Section~\ref{sec:underlyingsys}.  In section~\ref{sec:relatedwork}, we overview 
related work, and finally, in Section~\ref{sec:conclusion}, we conclude.

% Changes: I mentioned at the end that we need a static type checker to enforce
% our rules, and we need whole program analysis for that too.
\section{Abstract System Model}
\label{sec:model}
Our system model allows developers to split the domain of stored objects into a
collection of named, disjoint \emph{regions}. Each
region contains objects that are related by some application-specific integrity
invariant. The developer annotates each region with a consistency policy 
strong enough to enforce the implied invariants. Supported consistency policies 
must form a partial order based on strength, as defined
in~\cite{sivaramakrishnan2015declarative}. To interact with regions, the developer
declares \emph{actions}, sequences of atomic
read--write operations that perform a single logical task, and may interact with
multiple regions. An action succeeds if and only if all contained operations
succeed. The visibility of each write, and the recency of
data observed by each read within an action, for both failed and successful
actions, is determined by the consistency policy of the region with which reads
and writes interact. In this sense, actions are more flexible than relational
database transactions, which enforce a single isolation and visibility policy for all contained
operations.

This model allows controlled customization of the consistency policy of read operations; 
that is, it is allowed to request reads from a region at a weaker consistency level than the declared level of
the region. It is not allowed to read
at a consistency level stronger than that of the region
because it cannot be enforced without also changing the consistency
level of write operations. The model does not allow upward writes of data
derived from low consistency reads to regions with a higher consistency level.
However, an implementation may choose to
allow explicit promotion of low-consistency data to be written into higher
consistency regions, to increase the flexibility of the model.

Unlike reads, it is not possible to customize the consistency of write
operations. This is justified
by the assumption that each data region is annotated with a consistency policy strong enough to enforce 
any application-specific invariants. Selecting a stronger
consistency policy is not only pointless, but also can worsen
the availability or performance properties of the application. On the other hand,
selecting a weaker consistency policy makes it impossible to ensure that
application invariants are maintained by concurrent
operations. If a weaker policy is in fact strong enough to enforce any
application-specific
invariants, it should be used as the annotation on the data region itself. 
These data-centric write restrictions act as data region guards, because they 
force all writes to go though the needed consistency protocol.


%nesting of sets
%weaker containing stronger = ok
%stronger containing weaker = not ok
%merging of sets
%
%I need to specify the interface a store needs to support to be a suitable
%backend for our system, or specify the guarantees of the store.
%could just mention quelea

% TODO: the above is _way_ to vague. I don't know what our contribution is.
% I would still extend the model of [15]
% Something like:
% We have the ordering predicates on actions ww, wr, rw:
% ww(a,b): a writes x (version i), then b writes x (version i+1 [or j > i])
% wr(a,b): a writes x (version i), then b reads x (version i)
% rw(a,b): a reads x (version i), then b writes x (version i+1 [or j > i])
% serializability: ww, wr, rw are acyclic
% eventual consistency and MAV: as in [15]
% with vis = (ww U wr U rw)*  [I think vis is transitive; if not, remove the *]
% extend with causal consistency
% extend with transactions
% extend with sameregion predicates (cf. sameobject)

% Changes here: clarify the status of the application.
\section{Data-centric Cloud Types}
\label{sec:example}
In this section, we present a simplified subset of our implementation of the
proposed model, which is based on \emph{cloud
types}~\cite{burckhardt2012cloud}. In our language, \emph{DCCT} (Data-Centric
Cloud Types), the data-centric model rules are enforced through static type 
checking. Objects are integers or strings,
represented as replicated registers with last-writer-wins
semantics~\cite{burckhardt2014replicated}. At any point in time, different
versions of each entity may co-exist, either on different replicas or in local 
client buffers. Each version is assigned a time stamp as well as other metadata 
that might be needed to enforce a consistency property.

We define a partial order of three consistency policies, eventual
consistency (\texttt{EC}) $\prec$ monotonic atomic view (\texttt{MAV})~\cite{bailis2013highly}  $\prec$
serializability (\texttt{SR}), which apply on the action level. The programming language 
Quelea~\cite{sivaramakrishnan2015declarative} provides similar
consistency policies, as well as a proof of order validity. In short, consistency
levels are represented as first-order logical predicates; ordering is given
by entailment: stronger consistency policies entail weaker.
All variables in our implementation are at least eventually consistent
\texttt{EC}, which means it
is possible to freely observe any version of the object (without monotonicity
guarantees), and eventually all versions will converge. A region annotated 
\texttt{MAV} guarantees that all new versions written by an
action are observed together, and that version updates are observed
monotonically. The \texttt{SR} annotation on a
region guarantees that the latest version of all
contained objects is observed and manipulated at all times. 

\begin{figure}[t!] 
\begin{lstlisting}[basicstyle=\small\ttfamily]
class Customer {
  @SR username: String
  @EC name: String
  @EC gender: String
  @SR address: 
    (street: String, city: String, country: String) 
  @MAV paymentInfo:
    (paymentMethod: String, paymentInfo: String)
}
object mysteryProduct {
  @EC price: Int
  @SR amount: Int
}
class orderLog {
  @EC logRegion (
   username: String
   country: String
   date: String
   amount: Int
   orderSucceeded: Int 
   )
}
action orderMysteryProduct(
 customer: Customer, amount: Int) {
  val prevAmount = mysteryProduct.amount
  if (prevAmount - amount >= 0) {
    mysteryProduct.amount = prevAmount - amount
    submitOrder(
     amount, 
     customer.address, 
     mysteryProduct.price * amount)
    orderLog.insert(customer.username, 
      @EC customer.address.country, 
      Date.now(), amount, 1)
  } else {
    displayLowStockError() 
    orderLog.insert(customer.username, 
      @EC customer.address.country, 
      Date.now(), amount, 0)
  }
}
\end{lstlisting}
\caption{Example program in DCCT language with data-centric consistency policies}
\label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows a simple schema with annotated
regions and actions. Regions are annotated 
with their consistency policies. For simplicity, regions are simply fields,
which may be records containing multiple simple fields.
%
In this example, the developer requires \texttt{username}s to be 
unique, hence the field \texttt{Customer.username} is declared as serializable.
It is still possible to read \texttt{username}s at lower
consistency levels if needed (e.g., if we only need to display the name). 
\texttt{name} and \texttt{gender} are not very important, even if older
data is shown or intermediate data is written, the customer can still correct
it and eventually all replicas of this information will converge.
\texttt{address} is strongly consistency to avoid shipping to an old address.
Payment information is declared \texttt{MAV}, to avoid showing a mixed state
of payment info. Using \texttt{MAV} consistency
allows an older payment method to be used when charging the customer,
however, the developer is willing to accept this anomaly for the sake of
availability. \texttt{mysteryProduct.amount} is \texttt{SR} to ensure that all operations
observe the latest version and avoid selling non-existent items. While it is possible to 
request a read at a lower level, our model ensures that we cannot write back an amount 
based on such reads.
%
We show only one action \texttt{orderMysteryProduct}. This action does
not have a single isolation level, as is the case in regular transactions;
rather, the action manipulates data with many different consistency policies.
However, we can see the flexibility and safety this approach gives the
developer. It is possible to read and write at multiple consistency levels
seamlessly while preserving the consistency requirements of each region.

Actions in our system can be compared to generic functions interacting
with different data stores that enforce varying consistency guarantees.
\texttt{orderMysteryProduct} can be seen as a function that
reads and updates \texttt{mysteryProduct.amount}, stored in a
serializable relational database, and also stores operation logs in an eventually consistent,
key-value store. In addition, actions can signal success
or failure, and developers have to handle them appropriately. For brevity, we
omit failure handling details here.

% Fixes: I just removed failure handling details.
\section{Supporting Data-Centric Consistency}
\label{sec:underlyingsys}
The underlying storage system needed to support annotated regions and actions must
support at least the strongest consistency level, and ideally all consistency
levels specifiable in the language, with equivalent or stronger
guarantees. For the implementation described here, we use a customized
version of Kaiju~\cite{kaiju}, a prototype key-value store implemented to
evaluate~\cite{bailis2014scalable}. To implement \texttt{SR} regions, we
associate a pair of Kaiju's long read and write locks with each region. 
The relevant locks are acquired when an action interacting with an \texttt{SR}
region starts and released when action execution ends. 
To implement \texttt{MAV} regions, Kaiju provides
\texttt{get\_all} and \texttt{put\_all} functions, which execute a
RAMP~\cite{bailis2014scalable} algorithm
to fetch related values in a way consistent with our \texttt{MAV} consistency
definition. As described in \cite{bailis2014scalable}, at the beginning of
each action, a call to
\texttt{get\_all} is issued for each \texttt{MAV} annotated region.
The data fetched
is placed in a local cache. Subsequent writes in the action are performed on
the local cache. When the action completes successfully, a % why do we call them
%dirty regions?
\texttt{put\_all} call is issued on the local cache per region to write them back to the server. 
%
\texttt{EC} regions are read and written directly using the baseline protocol, 
which provides no concurrency control; the LWW semantics of our registers guarantee
eventual consistency of represented objects. 

The DCCT compiler translates actions to Java methods containing Kaiju calls.
The generated code can be made more efficient. For instance, specialized 
program analysis can deduce that  \texttt{SR}  region guarantees can be satisfied 
with short read or write locks. Our aim here
was to show the feasibility of implementing actions enforcing multiple
consistency properties.

\section{Related Work}
\label{sec:relatedwork}
 % Vaziri's atomic sets
Our approach is mainly inspired by Vaziri et al.~\cite{dolby2012data}. In their work, 
the Java language is extended with constructs that allow programmers to declare
\emph{atomic sets} that contain places in memory for which operations must be
serialized. The compiler inserts synchronization primitives for all
operations interacting with such sets to ensure linearizability over the
regions accessed by the operations. The main advantage they enlist is gathering
the all synchronization logic at one code location rather than all  control flow
paths leading to a memory operation on
shared data that must be dominated by a synchronization operation. 
Our model generalizes this idea, allowing consistency
levels other than linearizability.
% Kraska's consistency rationing
Kraska et al.~\cite{kraska2009consistency} have also proposed a data-centric
approach supporting multiple consistency levels. Besides 
serializability and eventual consistency, they also allow some data to have adaptable consistency and
accept invariant violations with some cost. While their aim is to reduce
such costs, ours is to maintain the consistency level required to enforce
certain invariants at all times, therefore simplifying reasoning about 
application
correctness. Furthermore, their transactions are not restricted from updating
data at lower consistency levels, which gives rise to anomalies.

Another line of work explores the relationship between
application correctness invariants and consistency levels required to enforce them.
% bailis I-confluence analysis 
Bailis el al.~\cite{bailis2014coordination} propose a criteria they call
\emph{I-confluence}, to determine whether enforcing certain invariants require distributed
coordination. Invariants that are I-confluent can be satisfied without strong
consistency. 
% Indigo putting consistency back, i-offender analysis
An extension of I-confluence is proposed by Balegas et
al.~\cite{balegas2015putting}, who introduce a framework that allows declaring
correctness invariants over data. However, the weakest supported consistency
model is causal consistency, and not all types of invariants can be
expressed.
% Cause I am strong enough, a proof rule relying on invariant declarations 
Gotsman et al.~\cite{gotsman2016cause} propose a proof rule, automated in an SMT-based
tool to show that a particular choice of consistency guarantees for operations
ensures the preservation of programmer-defined application invariants. As in
Balegas et al.~\cite{balegas2015putting}, causal consistency is the lowest supported model.
% Quelea language

The Quelea programming language~\cite{sivaramakrishnan2015declarative} allows
programmers to annotate operations with their consistency requirements
in the form of first-order logical predicates.
An SMT solver maps these requirements to the actual
consistency levels provided by the underlying system. Quelea's contracts are
lower-level than Gostman's, and do not ensure that the underlying integrity
invariants are maintained. Quelea's model reasons about higher-level
operations (e.g. deposit, withdraw), as opposed to our approach where we reason 
only about reads and writes. Hence, it is possible in Quelea for operations with
multiple consistency levels to interact with the same data, while we require 
operations to follow the consistency policy declared on data. Their 
approach is thus more flexible in this sense, however, it requires keeping track of the entire
history of operations, and describing all ordering and visibility properties among 
operations accessing the same data.

% Adya's mixing theorem
To reason about the correctness of systems with transactions running at different
isolation levels, Adya~\cite{adya2000generalized} proposed the mixing
theorem, which states that in mixing-correct histories, each transaction is provided 
the guarantees that pertain to its level. The implication is that transactions running 
at lower isolation levels must ``know what they are doing'', in terms of application 
invariants they must preserve, and must update the database consistently even if they observe an
inconsistent state. Our data-centric approach does not rely on operations
``knowing what they doing'' and instead forces all operations interacting with data
for which invariants must be preserved to follow the declared consistency
policy. 

\section{Conclusions, Current, and Future Work}
\label{sec:conclusion}
The main advantage of our approach, compared to other proposals, is its simplicity,
in terms of usage and description, and its ability to accommodate any
non-probabilistic consistency properties that form a partial order. On the down side, 
it requires whole program analysis, and does not attempt to prove that underlying 
invariants will be maintained, putting it at the hands of the developer to decide 
on the suitable consistency policy needed for each region. We believe that it is 
possible to combine our work with ~\cite{gotsman2016cause}
and ~\cite{balegas2015putting}, to show that a developer's selection of consistency
policies maintains underlying application invariants.
The data-centric consistency model presented here supports only 
disjoint regions. We plan to extend the model with support for nesting and merging 
of regions in the spirit of~\cite{dolby2012data}.
Temporarily merging regions is useful to avoid higher-level data races~\cite{artho2003high}.
Furthermore, we also plan to survey commonly used simple invariants in data-backed applications to support
declaring them directly, such as the uniqueness invariant~\cite{bailis2015feral},
which is easier  and more readable. To prove the soundness of the proposed model and 
programming language, we will formally specify the regions/actions model and their
interactions, as well as the typing rules and operational semantics of the
proposed language. Finally, a thorough evaluation, through implementing
real world applications and benchmarks is necessary to to evaluate the ease of use
and expressiveness of the language, as well as the performance of its generated
code.

%\appendix

\acks We thank Fernando Pedone and members of his research group
for very helpful discussions and useful pointers. We would also like to
thank our reviewers for their invaluable feedback.

%cknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{../NosheenZazaBib}


\end{document}
