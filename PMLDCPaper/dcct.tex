\documentclass[preprint,numbers]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{listings}
\usepackage{txtt}
\usepackage{color}
\usepackage{defs}
\usepackage{hyperref}

\newcommand{\cL}{{\cal L}}

%TODO
% 1) find a way to handle nestings, check the work of atomic sets and loci again
% this was my problem because of assuming that nested levels of consistency in
% an operation results imply the same things when talking about nested data
% regions.
% 2) define all version combinations for a number of cosistency policies, make
% it clear what can and cannot be supported.
% so in theory everything can be defined in terms of current versions, and how
% next versions relate to each other in a set. 
% 3) find a way to combine this with the mixing theorem of Adya, 
% I state that my system does not assume transactions know what they are doing,
% and also makes it clear that we need to write at minimum isolation level
% declared on a set.
% TODO what is the state in database systems for atomicity of individual sql
% statements? Can we guarantee isolation level 3 if sql statements are not
% atomic?
% TODO describe handlind deletions and predicate operations over entities later,
% or mention it as future work.
% TODO should I have set policies for "relations"? or not as they do not exist
% in distributed stores?
% TODO  look again at sagas

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PMLDC '16}{July 17, 2016, Rome, , Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Data-centric Consistency Policies} % 'preprint' option specified.

\title{Data-centric Consistency Policies}
\subtitle{A Programming Model for Distributed Applications with Tunable Consistency}

\authorinfo{Nosheen Zaza}
           {Universit\`a della Svizzera italiana (USI)}
           {nosheen.zaza@usi.ch}
\authorinfo{Nathaniel Nystrom}
           {Universit\`a della Svizzera italiana (USI)}
           {nate.nystrom@usi.ch}

\maketitle

% Abstract change: mention that there is support for for reasoning about 
% multiple consistency levels in research, however the frameworks are
% complicated, ours is good in that it is simpler. 
\begin{abstract}
The consistency level of operations over replicated data is 
an important parameter in distributed applications. It impacts correctness, 
performance, and availability. It is now common to find single applications
using many different consistency
levels at the same time; however, current commercial frameworks do not
provide high-level abstractions for specifying or reasoning about different
consistency properties of an application. Research frameworks that do tend to require a
substantial effort from developers to specify operation dependencies, orderings
and invariants to be preserved.
We propose an approach for specifying consistency properties based on the
observation that
correctness criteria and invariants are a property of data, not operations.
Hence, it is reasonable to define the consistency properties
required to enforce various data invariants on the data itself rather than on the operations. 
The result is a system that is simpler to describe and reason about. 
In this paper, we outline an abstract model
for data-centric consistency specification, and demonstrate it through a simple,
concrete programming language prototype implementation.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
distributed systems, eventual consistency, domain-specific programming languages

\section{Introduction}
In modern distributed systems, data is often replicated 
to enhance availability and resilience to failure, and to improve performance by
placing replicas closer to clients.
However, managing the consistency of 
replicated data is a nontrivial problem. Ideally, all clients should observe
the most recent data at all times.
Achieving this consistency level through
linearizability~\cite{herlihy1990linearizability} or
serializability~\cite{papadimitriou1979serializability} entails
well-documented costs in terms of latency, availability, and partition
tolerance~\cite{bailis2013highly, kraska2013mdcc}. For this reason,
many weaker consistency levels have been proposed,
such as eventual consistency~\cite{vogels2009eventually}, causal
consistency~\cite{lamport1978time, schiper1989new}, and snapshot isolation~\cite{kemme2000database}.
The choice of one of the above consistency levels depends on application
correctness and performance requirements. For instance, in a flight booking
application, there is no way eventual consistency alone
can guarantee that a single airplane seat will not be assigned to two different
people~\cite{bailis2014coordination, bailis2013highly}. However, eventual
consistency works well
in many non-critical cases, for example, creating and showing product
recommendations, and is widely used in practice~\cite{eventualeBay,eventualNetflix,hastorun2007dynamo}.

In current systems, to enforce a certain consistency
property in an application, a developer selects a storage system that
supports this property, then specifies the consistency level per
operation or per
sequence of operations. However, mapping high-level application consistency requirements to a combination of
low-level consistency settings on operations is often tedious and error-prone.
% Low priority: the for the first problem, we really do not do much 
% to solve it, I should really say that achieveing consistency with operations
% opererating at different consistency levels is not straight forward.
For instance, Cassandra~\cite{lakshman2010cassandra} and Riak~\cite{riak} allow 
specifying one of many consistency levels:
\texttt{one}, \texttt{two}, \texttt{quorum}, \texttt{all}, etc., describing
the set of nodes that need to
acknowledge the success of an operation. It
is not immediately clear how consistency levels such as causal consistency or
linearizability can be achieved, if at all, using these systems. The Cassandra documentation
states that,
to achieve ``strong consistency''\footnote{Strong consistency here implies
neither linearizability or serializability, as explained 
in~\cite{sivaramakrishnan2016representation}.} clients must issue quorum reads and writes,
however, nothing prevents a misbehaving client from issuing concurrent \texttt{one} writes and
\texttt{quorum} reads on the same data, thus violating strong consistency for other clients. Another
problem is when a client issues \texttt{all} reads and writes, which is completely
unnecessary for enforcing any high-level consistency policies. Similar issues
occur in relational database systems, where it cannot be guaranteed that a
sequence of transactions running at different isolation levels preserves the intended
invariants~\cite{gray1992transaction}. 

% TODO the solution is not only for safety, but also for expressiveness. 
To tackle such problems, we propose a data-centric, declarative programming
model inspired by~\cite{dolby2012data}, where
high-level consistency requirements are declared for data regions that share a
consistency property, and low-level consistency settings for operations interacting with
regions are assigned or restricted accordingly. We
describe the abstract model in Section~\ref{sec:model}, and in
Section~\ref{sec:example} we motivate its usefulness in addressing the above issues by showing
an example in a prototype language implementing this model\footnote{This
  language is a work in progress. The project that can be found at
  \url{https://github.com/amanjpro/languages-a-la-carte/}}.
In Section~\ref{sec:conclusion} we conclude.

% Changes: I mentioned at the end that we need a static type checker to enforce
% our rules, and we need whole program analysis for that too.
\section{Abstract System Model}
\label{sec:model}
Our system model allows developers to split the domain of stored objects into a
collection of named, disjoint
\emph{regions}. Each
region contains objects that are related by some application-specific consistency property. The
developer annotates each region with a consistency level 
strong enough to enforce the implied invariants. The consistency levels form
a partial order
based on strength, as defined
in~\cite{sivaramakrishnan2015declarative}.
To interact with regions, the developer
declares \emph{actions}, sequences of atomic
read--write operations that perform a single logical task, and may interact with
multiple regions. An action succeeds if and only if all contained operations
succeed. The visibility of each write, and the recency of
data observed by each read within an action, for both failed and successful
actions is determined by the consistency policy of the region with which reads
and writes interact. In this sense, actions are more flexible than relational
database transactions, which enforce a single isolation and visibility policy for all contained
operations. It is not possible to customize the consistency of any write operation, justified
by the assumption that each data region is annotated with a consistency policy strong enough to enforce 
an implied invariant; selecting a stronger
consistency policy is not only pointless, but also can worsen
availability or performance properties of the application. On the other hand,
selecting a weaker consistency policy makes it impossible to prove that
application invariants will appear satisfied for all concurrent
operations. If a weaker policy is in fact strong enough to enforce all implied
invariants, it should be used as an annotation on the data region itself. 
These data-centric write restrictions act as data region guards, because they force all writes to go
though the needed consistency protocol. For read operations, we allow controlled customization of 
their consistency level; it is still not allowed to request reading
at a consistency level stronger than that of the region containing the item
being read, again because it cannot be guaranteed given our restrictions on
write consistency levels, but it is possible to
request reading at a lower consistency level. In this case, however, we do not allow
upward writes of data derived from low
consistency reads to regions with a higher consistency level. An implementation may choose to
allow explicit promotion of low-consistency data to be written into higher
consistency regions to increase the flexibility of the model. A framework that
supports this model enforces its rules through static type checking, to ensure
no upward writes exist, and no data items stores in regions escape actions,
which is necessary to ensure the correctness of the system, as explained
in~\cite{burckhardt2012cloud}, which proposes a similar system that separates
local types from replicated, distributed types. {@nate actually going back to that, I forgot 
why this was necessary}
%nesting of sets
%weaker containing stronger = ok
%stronger containing weaker = not ok
%merging of sets
%
%I need to specify the interface a store needs to support to be a suitable
%backend for our system, or specify the guarantees of the store.
%could just mention quelea

% TODO: the above is _way_ to vague. I don't know what our contribution is.
% I would still extend the model of [15]
% Something like:
% We have the ordering predicates on actions ww, wr, rw:
% ww(a,b): a writes x (version i), then b writes x (version i+1 [or j > i])
% wr(a,b): a writes x (version i), then b reads x (version i)
% rw(a,b): a reads x (version i), then b writes x (version i+1 [or j > i])
% serializability: ww, wr, rw are acyclic
% eventual consistency and MAV: as in [15]
% with vis = (ww U wr U rw)*  [I think vis is transitive; if not, remove the *]
% extend with causal consistency
% extend with transactions
% extend with sameregion predicates (cf. sameobject)

% Changes here: clarify the status of the application.
\section{Sample Implementation and Example}
\label{sec:example}
We now describe a subset of a proof of concept implementation we are currently
working on, codenamed \emph{dcct} language.
The objects in our system are integers or strings,
represented as replicated registers with last-writer-wins
semantics~\cite{burckhardt2014replicated}. At any point in time, different
versions of each entity may
co-exist, either on different replicas or in local client buffers. Each version
is assigned a time stamp as well as other metadata that might be needed to
enforce a consistency property. We define a partial order of three consistency properties, eventual
consistency (\texttt{EC}) $\prec$ monotonic atomic view (\texttt{MAV})~\cite{bailis2013highly}  $\prec$
serializability (\texttt{SR}), which apply on the action level.\footnote{The programming language Quelea~\cite{sivaramakrishnan2015declarative} provides similar
consistency levels, as well as a proof of order validity. In short, consistency
properties are represented as first-order logic predicates, a weaker consistency
model is described with less logic predicates than a stronger model, and allows
more permutations of concurrent operations.} 
All objects in our implementation are at least eventually consistent by default, which means it
is possible to freely observe any versions (without monotonicity guarantee), and
eventually all versions converge. A region annotated \texttt{MAV} guarantees all new versions written by an
action are observed together, and that version updates are observed
monotonically. The \texttt{SR} annotation on a
region guarantees that the latest version of all
contained objects is observed and manipulated at all times. 

\begin{figure}[t!] 
\begin{lstlisting}[basicstyle=\small\ttfamily]
class Customer {
  @SR username: String
  @EC name: String
  @EC gender: String
  @SR address: 
    (street: String, city: String, country: String) 
  @MAV paymentInfo:
    (paymentMethod: String, paymentInfo: String)
}
object mysteryProduct {
  @EC price: Int
  @SR amount: Int
}
class orderLog {
  @EC username: String
  @EC country: String
  @EC date: String
  @EC amount: Int
  @EC orderSucceeded: Int
}
action orderMysteryProduct(
 customer: Customer, amount: Int) {
  val prevAmount = mysteryProduct.amount
  if (prevAmount - amount >= 0) {
    mysteryProduct.amount = prevAmount - amount
    submitOrder(
     amount, 
     customer.address, 
     mysteryProduct.price * amount)
    orderLog.insert(customer.username, 
      @EC customer.address.country, 
      Date.now(), amount, 1)
  } else {
    displayLowStockError() 
    orderLog.insert(customer.username, 
      @EC customer.address.country, 
      Date.now(), amount, 0)
  }
}
\end{lstlisting}
\caption{Example program in dcct language with data-centric consistency policies}
\label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows a simple schema with annotated
regions and actions. Regions are annotated 
with their consistency level. For simplicity, regions are simply fields,
which may be records containing multiple simple fields.
In this example, the developer decided that \texttt{username}s must be 
unique, hence serializability is needed
when allocating them. It is still possible to read usernames at lower
consistency levels if needed (e.g., if we only need to display the name). 
\texttt{name} and \texttt{gender} are not very important, even if older
data is shown or intermediate data is written, the customer can still correct
it and eventually all replicas of this information will converge.
\texttt{address} is strongly consistency to avoid shipping to an old address.
Payment information is declared \texttt{MAV}, to avoid showing a mixed state of payment info, this still
imposes the problem of using an older payment method when charging the customer,
however, the developer is willing to accept this anomaly for the sake of
availability. \texttt{mysteryProduct.amount} is \texttt{SR} to ensure that all operations
observe the latest version and avoid selling non-existent items. While it is possible to request a read at a lower
level, our model ensures that we cannot write back an amount based on such reads. For brevity,
we show only one action \texttt{orderMysteryProduct}. This action does
not have a single isolation level, as is the case in regular transactions;
rather, the action manipulates data with many different consistency levels.
However, we can see the flexibility and safety this approach gives the
developer. It is possible to read and write at multiple consistency levels
seamlessly while preserving  consistency requirements for each region.

Actions in our system can be compared to generic functions interacting
with different data stores that enforce varying consistency guarantees.
\texttt{orderMysteryProduct} can be seen as a function that
reads and updates \texttt{amount} stored in a relational database
transactionally (e.g., read and write locks are acquired on \texttt{amount} until
action success is signaled), and also stores operation logs in an eventually consistent,
key-value store. Actions provide the additional property of signaling success
of failure, and developers have to handle them appropriately. For brevity, we
omit failure handling details here, the interested reader can refer to(@nate I
could not find a very specific reference, any suggestions?)

\section{Underlying System Features}
\label{sec:underlyingsys}
The underlying storage system needed to support annotated regions and actions should
support at least the strongest consistency level, and ideally all consistency
levels described in the language, with equivalent or stronger
guarantees. For the particular implementation described here, we use a customized
version of Kaiju~\cite{kaiju}, which is a prototype key-value store implemented to
evaluate~\cite{bailis2014scalable}. For our purpose, it provides long read and
write locks. Each lock associated with a region touched by an action is acquired
at when the action starts, and released upon success, upon failure, it is
released after restoring the previous database state. As for \texttt{MAV}
regions touched by an action, Kaiju provides
\texttt{get\_all} and \texttt{put\_all} functions, which execute a ramp algorithm
to fetch related values in a way consistent with our \texttt{MAV} consistency
definition. As described in ~\cite{bailis2014scalable} concerning implementing
read-write transactions, at the beginning of each action, a call to
\texttt{get\_all} is issued per \texttt{MAV} annotated region, the data fetched
is placed in a local cache. Each subsequent write to such regions is issued
against the local cache, at the end of transaction, upon commit, a
\texttt{put\_all} call
is issued for every region with the values form the cache. As for \texttt{EC}
regions they are read and written directly using the baseline protocol, which
provides to concurrency control. If any store-related operation fails, the
entire action is marked failed, in which case the store needs to be restored as
needed to satisfy the consistency policy of each region, in addition to rolling
back any changes made by uncommitted actions. A compiler is needed to translate
actions as described to a Java program containing Kaiju calls. The generated code can be made more 
efficient. For instance, specialized program analysis can deduce that serial 
region guarantees can be  satisfied with short read or write locks. Our aim here
was to show the feasibility of implementing actions enforcing multiple
consistency properties. 

\section{Related Work}
\label{sec:relatedwork}
 % Vaziri's atomic sets
Our approach is mainly inspired by Vaziri et al.~\cite{dolby2012data}. In their work, 
the Java language is extended with constructs that allow programmers to declare
\emph{atomic sets} that contain places in memory for which operations must be
serialized. An extended compiler then inserts synchronization primitives for all
operations interacting with such sets to ensure linearizability over contained
data. Our model generalizes this idea, to allow attaching consistency
properties other than linearizability to sets/regions.
% Kraska's consistency rationing
Kraska et al.~\cite{kraska2009consistency} have also proposed a data-centric
approach supporting multiple consistency levels. Other than supporting
serializability and eventual consistency, they allow some data to have adaptable consistency, and
accept invariant violations at a monetary cost. While their aim is to reduce
such costs, ours is to maintain the consistency level required to enforce
certain invariants at all times, hence simplifying reasoning about the application
correctness. Furthermore, their transactions are not restricted from updating
data with certain consistency attached at a lower consistency level, which gives
rise to anomalies, because they do not propose a static checker. 
Another related line of work explores the relationship between
application correctness invariants and consistency levels required to enforce them.
% bailis I-confluence analysis 
Bailis el al.~\cite{bailis2014coordination} propose a criteria they call
\emph{I-confluence}, to determine whether enforcing certain invariants require distributed
coordination. Invariants that are I-confluent can be satisfied without strong
consistency. 
% Indigo putting consistency back, i-offender analysis
An extension of I-confluence is proposed by Balegas et
al.~\cite{balegas2015putting}, and they propose a framework that allows declaring
correctness invariants over data. However,the weakest supported consistency
model is causal consistency, and not all types of invariants can be
expressed.
% Cause I am strong enough, a proof rule relying on invariant declarations 
Gotsman et al.~\cite{gotsman2016cause} propose a proof rule to show that a 
particular choice of consistency guarantees for operations
ensures the preservation of programmer-defined application invariants. Like
Balegas aforementioned work, causal consistency is the lowest supported model.
% Quelea language
The Quelea programming language~\cite{sivaramakrishnan2015declarative} allows
programmers to annotate operations with their required consistency requirements
in the form of first-order logic predicates, that describe fine-grained consistency
requirements. Then an SMT solver maps these requirements to the actual
consistency levels provided by the underlying system. Quelea's contracts are
lower-level than Gostman's, and do not ensure that the underlying integrity
invariants are maintained. Quelea's model reasons about higher-level
operations (e.g. deposit, withdraw), as opposed to our approach where we reason 
only about reads and writes. Hence, it is possible in Quelea for operations with
multiple consistency levels to interact with the same data, while we require all 
operations to follow the exact consistency policy declared on data. While their 
approach is more flexible in this sense, it requires keeling track of the entire
history of operations, and describing all ordering and visibility properties among 
operations accessing the same data.


% Adya's mixing theorem
%Adya desrcibes the mixing
%theorem, which desribes  transactions with multiple consistnecy
%policies must interact and behave to preseve the isolation requirements for all
%of them. The implication is that transactions running at lower isolation levels
%must "know what they are doing", in terms of application invariants they must
%preserve. Our approach facilitates reasoning about such systems, because data
%for which invariants must be preseved is protected by a suitable consistency
%policy, hence eliminating the risk of having transactiosn with unsuitable, it
%also gathers the reasoning in one place, instead of a combinations of policies
%spread over all operations accessing the data. 


The main advantage of our approach, compared to other proposals, is its simplicity,
in terms of usage and description, and its ability to accommodate any
non-probabilistic consistency properties that form a partial order. On the down side, 
it requires whole program analysis, and does not attempt to prove that underlying 
invariants will be maintained, putting it at the hands of the developer to decide 
on the suitable consistency policy needed for each region. We believe that it is 
possible to combine our work with ~\cite{gotsman2016cause}
and ~\cite{balegas2015putting}, to show that developers selection of consistency
policies maintains underlying application invariants


\section{Conclusions, Current and Future Work}
\label{sec:conclusion}
We have presented here a model that supports only disjoint regions. We plan to
extend the model with support for nesting and merging of regions in the spirit of~\cite{dolby2012data}.
Temporarily merging regions is useful to avoid higher-level data races~\cite{artho2003high}. We are
currently developing a programming language (dcct: Data Centric Cloud Types) and runtime with 
support for this extended model on top of cloud
types~\cite{burckhardt2012cloud}.

%\appendix

\acks We would like to thank prof. Fernando Pedone and his research group
members for very helpful discussions and useful pointers. We would also like to
thank our reviewers for their invaluable feedback.

%cknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{../NosheenZazaBib}


\end{document}
