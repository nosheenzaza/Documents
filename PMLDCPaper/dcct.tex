\documentclass[preprint, numbers]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{listings}

\newcommand{\cL}{{\cal L}}

%TODO
% 1) find a way to handle nestings, check the work of atomic sets and loci again
% this was my problem because of assuming that nested levels of consistency in
% an operation results imply the same things when talking about nested data
% regions.
% 2) define all version combinations for a number of cosistency policies, make
% it clear what can and cannot be supported.
% so in theory everything can be defined in terms of current versions, and how
% next versions relate to each other in a set. 
% 3) find a way to combine this with the mixing theorem of Adya, 
% I state that my system does not assume transactions know what they are doing,
% and also makes it clear that we need to write at minimum isolation level
% declared on a set.
% TODO what is the state in database systems for atomicity of individual sql
% statements? Can we guarantee isolation level 3 if sql statements are not
% atomic?
% TODO shorten abstract, Nate says too wordy.
% TODO future work: nesting and intersection of consistent regions.
% TODO think whether or not to include causal consistency.
% TODO describe handlind deletions and predicate operations over entities later,
% or mention it as future work.
% TODO should I have set policies for "relations"? or not as they do not exist
% in distributed stores?
% TODO  look again at sagas

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Data-centric Consistency Policies} % 'preprint' option specified.

\title{Data-centric Consistency Policies}
\subtitle{A theory and programming model for distributed applications}

\authorinfo{Nosheen Zaza}
           {Universit\`a della Svizzera italiana (USI)}
           {nosheen.zaza@usi.ch}
\authorinfo{Nathaniel Nystrom}
           {Universit\`a della Svizzera italiana (USI)}
           {nate.nystrom@usi.ch}

\maketitle

\begin{abstract}
The consistency level specified for operations over replicated data is 
an important parameter in distributed applications, as it impacts correctness, 
performance and availability. Nowadays, it is common to find various consistency
levels used in a single application, however, current frameworks do not
provide high-level abstractions for specifying or reasoning about different consistency properties of an application. 
We propose a new approach for specifying consistency properties based on an important observation: 
correctness criteria and invariants are a property of data, not operations, hence it is 
reasonable to enable defining the consistency properties required to enforce
data invariants on data itself rather than operations. In this paper, we overview an abstract model
for data-centric consistency specification, and demonstrate it through a simple,
concrete programming language implementation. \end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
distributed systems, domain-specific programming languages.

\section{Introduction}
In modern distributed systems, data is often replicated 
to enhance availability and resilience to failure, and improve performance by
placing data replicas closer to clients. However, managing the consistency of 
replicated data is a nontrivial problem. Ideally, all clients must observe the most recent, correct data at all time 
Achieving this consistency level through linearizability(ref) or
serializability(ref) entails well
documented costs in terms of latency, availability and partition
tolerance(refs). For this reason, many weaker consistency levels have been proposed,
such as eventual consistency(ref), causal consistency(ref), atomic
visibility(ref), and others(ref). 

The choice of one of the above consistency levels depends on application
correctness and performance requirements. For instance, there is no way eventual consistency
can guarantee that a single airplane seat will not be assigned to two different
people(ref), or that differnet products will be assigned unique, universally
increasing IDs(bailis, and check the exact case). However, it would work just fine
in many non critical cases, such as recommending products, or counting users
interested in a certain event. In order to achieve a certain consistency
property in an application, a developer would typically select a storage system that
actually supports this level, then specify a consistency level per operation or
sequences of operations. However, mapping high-level application consistency requirements to  a combination of
low-level consistency settings on operations is a tedious and error-prone task, especially if
the backing store supports multiple consistency levels. For instance,
Cassandra(ref) and Riak(ref) allow specifying one of consistency levels
\texttt{one}, \texttt{two}, \texttt{quorum}, \texttt{all}... etc, describing the number of nodes that need to
acknowledge the success of a read or write. It
is not immediately clear how consistency levels such as causal consistency or
linearizability can be achieved, if at all.  Cassandra documentation state that,
to achieve "strong consistency"\footnote{ Strong consistency here implies
neither linearizability or serializability, as explained in (quelea ref)} clients must issue quorum reads and writes,
however, nothing prevents a misbehaving client from issuing concurrent \texttt{one} writes and
\texttt{quorum} reads on the same data, thus violating strong consistency for other clients. Another
problem is when a client issuing \texttt{all} reads and writes. Which is completely
unnecessary for enforcing any high-level consistency policies. Similar issues
can occur in relational database systems, as it cannot be guaranteed that a
sequence of transactions running at different isolation level preserve intended invariants(ref). 

% TODO the solution is not only for safety, but also for expressiveness. 
The core cause of such problems is that current storage systems allow arbitrary
interactions among low-level operations executing at different consistency levels. To
tackle this problem, we propose a data-centric, declarative programming model, where
consistency requirements are declared for data regions over which a correctness
invariant must hold; and the low-level consistency settings for operations interacting with
a region are assigned or restricted accordingly. In the rest of this paper, we describe this
model and motivate its usefulness in addressing the above issues, then show
an example in a prototype language implementing this model.

\section{Abstract System Model}
Our system model allows developers to split the domain of stored objects into a
collection of named, disjoint
regions \texttt{Set s = \{x, y, z..\}}. Each
region contains objects that are related by a correctness invariant. The
developer annotates each region with a consistency level \texttt{c} $\in$
\texttt{ConsistencyClass}, where \texttt{ConsistencyClass} is a total order of
consistency definitions based on strength, as defined in(ref). The system assumes that each
set is annotated with the weakest consistency level capable of
enforcing the implied invariant. To interact with regions, the developer
declares \emph{actions}: \texttt{action a = \{o1, o2...\}} sequences of atomic
read/write operations that perform a single logical task.
Operations in the same action may interact with multiple sets. An attempt to
execute an action may succeed or fail, where an action succeeds if and only if
all contained operations succeed. A consistency policy on a region applies only to
all operations in an action interacting with it, as opposed to a complete
history of actions. Hence it is important to
correctly group related operations into a single action. The visibility
of each write within an action, for both failed and successful
actions is determined by the consistency policy of the region it writes to. It
is not possible to customize the consistency of any write operation, justified
by the assumption that each data region is annotated with the weakest
consistency set strong enough to enforce an invariant, then selecting a stronger
consistency policy us not only pointless, but also can worsen
availability or performance properties of the application. On the other hand,
selecting a weaker consistency policy makes it impossible to prove that
application invariants will appear satisfied for all concurrent operations.
While these assumptions are indeed conservative, they ensure that no accidental
mis-specification of consistency policies will lead to errors. In this way, the
data-centric policies act as data guards, because they force all writes to go
though the needed consistency protocol. Furthermore, preventing writes at higher
consistency levels ensure that desired availability and partition tolerance
properties are clear and maintained. 

What about reads? Similarly, the consistency of fetched data is determined by the
policy of the region it was read from. However, unlike writes, it is possible to
customize the consistency level for reads. It is still not allowed to request reading
at a consistency level stronger than that of the region containing the item
being read, again because it cannot be guaranteed given our restrictions on
write consistency levels, however, it is possible to
request reading at a lower consistency level. After a read, the local memory
region to which the data have been read is tagged with the consistency policy of
the source region. To provide another layer of safety, the model does not allow
writing data fetched, or generated based on data fetched from regions with low
consistency to regions with higher consistency. An implementation may choose to
allow explicit promotion of low-consistency data to be written into higher
consistency regions to increase the flexibility of the model.

%nesting of sets
%weaker containing stronger = ok
%stronger containing weaker = not ok
%merging of sets
%
%I need to specify the interface a store needs to support to be a suitable
%backend for our system, or specify the guarantees of the store.
%could just mention quelea


\section{Sample Implementation and Example}
We now describe a proof of concept implementation, showing only
relevant key points. We do not claim this implementation to be of any practical
use, however, it serves to demonstrate our model in simple terms, and present some examples.

The objects in our system are integers or strings, represented as replicated registers with LWW
semantics(ref). At any point of time, different versions of each entity can
co-exist, either on different replicas or on local client buffers. each version
is assigned a timestamp as well as other metadata that might be needed to
enforce a consistency property. We define a partial order of three consistency properties, eventual
consistency \texttt{ec} $<$ monotonic atomic view \texttt{mav} $<$
serializability\footnote{The programming language Quelea(ref) provides similar
  consistency levels, as well as proof of order validity}. All
objects in our implementation are at least eventually consistent by default, which means it
is possible freely observe any versions (without monotonicity guarantee),
however, these versions will eventually converge. \texttt{mav} and \texttt{sc}
can be used as region annotations. A region with \texttt{mav} annotation guarantees that all contained objects
are observed at the same revision, and revisions are updated monotonically in
terms of versions. \texttt{sc} on a region guarantees that the last version of all
contained objects is observed at all times. 

The following listing shows a simple schema with annotated regions and actions:
\begin{lstlisting}[basicstyle=\small]
table Customer {
   @sc username: string
   name: string 
   country: string
   @sc region address: 
     (city: string, street: string)
   @mav region paymentInfo 
     (paymentMethod: string, paymentInfo: string)

}
entity mysteryProduct {
  price: int
  @sc amount: int
}
table opLog {
  username: string
  country: string
  date: string
  amount: int
  orderSucceeded: int
}
action orderMysteryProduct(customer: Customer, amount: string) {
  val prevAmount = mysteryProduct.amount
  if(prevAmount - amount >= 0 )
    mysteryProduct.write(prevAmount - amount)
    submitOrder(
     amount, 
     address(customer.city, customer.street), 
     mysteryProduct.price * amount )
     opLog.insert(customer.username, 
        customer.country, date.now(), 1)
  else 
    ERROR
    opLog.insert(customer.username, 
        customer.country, date.now(),, 0)
}
\end{lstlisting}

A developer (who apparently does not care much about rare customer inconvenience)
decided that \texttt{username}s must be unique, hence serializability is needed
when allocating them. \texttt{name} and \texttt{country} are not very important, even if older
data is shown or intermediate data is written, the customer can still correct
the data and eventually all replicas of these information will converge.
\texttt{address} is strongly consistency in order to avoid shipping to an old address.
payment information is \texttt{mav}, to avoid showing a mixed state of payment info, this still
imposes the problem of using an older payment method when charging the customer,
however, the developer is willing to accept this anomaly for the sake of
availability. 

we have omitted actions needed to create customers, or increase the stock of mystery
product. Since \texttt{amount} is \texttt{sc} consistent on action level, and isolated as
a until action execution is finished, we guarantee that the latest version of
amount is always observed and updated for all actions interacting with it. 

Action \texttt{orderMysteryProduct} is interesting because it does
not have a single isolation level as is the case in regular transactions.
However, we can see the flexibility and safety this approach gives the
developer. It is possible to read and write at multiple consistency levels
seamlessly, and it is correct according to consistency region specifications.
Actions in our system can be compared to functions or procedures interacting
with different data stores enforcing varying consistency guarantees. For
instance, we can rethink of \texttt{orderMysteryProduct}  as a function that
reads and updates \texttt{amount} stored in a relational database
transactionally, and also stores operation logs in an eventually consistent,
key-value store. our actions are different in that, if any operation fails
within an action, the entire action fails, and the programmer has to handle this
failure appropriately. For brevity, we omit failure handling details here.

\section{Conclusions and Future Work}
Currently, we only support declaring disjoint regions. However, the ability to have
nested regions is necessary to allow expressive and effecient implementations, 
and we are currently working to support this feature.
In general, it should be allowed to nest sets with stronger consistency within a
set with weaker consistency, but not the other way around if we are to guarantee
that the consistency of the compound region can be achieved. Another feature we
are working to support is allowing actions to temporarily merge regions, which
is needed to avoid higher-level data races(ref). Our approach will be analogous
to(ref atomic sets), with restrictions of possible consistency of the resulting
and component sets. 
We are currently implementing a language cased on Cloud Types language(ref), that incorporates
regions and actions, and we plan to evaluate it be encoding benchmarks such as
TPC-C and TPC-W.

%\appendix

%\acks

%cknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}


\end{document}
