\documentclass[preprint,numbers]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% numbers       To obtain numeric citation style instead of author/year.

\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}
\usepackage{defs}

\newcommand{\cL}{{\cal L}}

%TODO
% 1) find a way to handle nestings, check the work of atomic sets and loci again
% this was my problem because of assuming that nested levels of consistency in
% an operation results imply the same things when talking about nested data
% regions.
% 2) define all version combinations for a number of cosistency policies, make
% it clear what can and cannot be supported.
% so in theory everything can be defined in terms of current versions, and how
% next versions relate to each other in a set. 
% 3) find a way to combine this with the mixing theorem of Adya, 
% I state that my system does not assume transactions know what they are doing,
% and also makes it clear that we need to write at minimum isolation level
% declared on a set.
% TODO what is the state in database systems for atomicity of individual sql
% statements? Can we guarantee isolation level 3 if sql statements are not
% atomic?
% TODO shorten abstract, Nate says too wordy.
% TODO future work: nesting and intersection of consistent regions.
% TODO think whether or not to include causal consistency.
% TODO describe handlind deletions and predicate operations over entities later,
% or mention it as future work.
% TODO should I have set policies for "relations"? or not as they do not exist
% in distributed stores?
% TODO  look again at sagas

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PMLDC '16}{July 17, 2016, Rome, , Italy}
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Data-centric Consistency Policies} % 'preprint' option specified.

\title{Data-centric Consistency Policies}
\subtitle{A Theory and Programming Model for Distributed Applications}

\authorinfo{Nosheen Zaza}
           {Universit\`a della Svizzera italiana (USI)}
           {nosheen.zaza@usi.ch}
\authorinfo{Nathaniel Nystrom}
           {Universit\`a della Svizzera italiana (USI)}
           {nate.nystrom@usi.ch}

\maketitle

\begin{abstract}
The consistency level specified for operations over replicated data is 
an important parameter in distributed applications. It impacts correctness, 
performance, and availability. It is now common to find single applications
using many different consistency
levels at the same time; however, current frameworks do not
provide high-level abstractions for specifying or reasoning about different consistency properties of an application. 
We propose an approach for specifying consistency properties based on the
observation that
correctness criteria and invariants are a property of data, not operations.
Hence, it is reasonable to define the consistency properties
  required to enforce various data invariants on the data itself rather
  than on the operations. In this paper, we outline an abstract model
for data-centric consistency specification, and demonstrate it through a simple,
concrete programming language implementation.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
%\terms
%term1, term2

\keywords
distributed systems, domain-specific programming languages.

\section{Introduction}
In modern distributed systems, data is often replicated 
to enhance availability and resilience to failure, and to improve performance by
placing replicas closer to clients.
However, managing the consistency of 
replicated data is a nontrivial problem. Ideally, all clients should observe
the most recent data at all times.
Achieving this consistency level through
linearizability~\cite{herlihy1990linearizability} or
serializability~\cite{papadimitriou1979serializability} entails well
documented costs in terms of latency, availability, and partition
tolerance~\cite{bailis2013highly, kraska2013mdcc}. For this reason,
many weaker consistency levels have been proposed,
such as eventual consistency~\cite{vogels2009eventually}, causal
consistency~\cite{lamport1978time}, and snapshot isolation~\cite{kemme2000database}.
The choice of one of the above consistency levels depends on application
correctness and performance requirements. For instance, in a flight booking
application, there is no way eventual consistency alone
can guarantee that a single airplane seat will not be assigned to two different
people~\cite{bailis2014coordination, bailis2013highly}. However, eventual
consistency works well
in many non-critical cases, e.g., creating and showing recommended products
lists, and is widely used in practice~\cite{???}.

To enforce a certain consistency
property in an application, a developer should select a storage system that
actually supports this property, then should specify the consistency level per
operation or
sequences of operations. However, mapping high-level application consistency requirements to a combination of
low-level consistency settings on operations is often tedious and error-prone, especially if
the backing store supports multiple consistency levels~\cite{???}.
For instance,
Cassandra~\cite{lakshman2010cassandra} and Riak~\cite{???} allow specifying one of many
consistency levels
\texttt{one}, \texttt{two}, \texttt{quorum}, \texttt{all}, etc., describing the number of nodes that need to
acknowledge the success of an operation. It
is not immediately clear how consistency levels such as causal consistency or
linearizability can be achieved, if at all, using these systems.  The Cassandra documentation
states that,
to achieve ``strong consistency''\footnote{Strong consistency here implies
neither linearizability or serializability, as explained in~\cite{sivaramakrishnan2016representation}} clients must issue quorum reads and writes,
however, nothing prevents a misbehaving client from issuing concurrent \texttt{one} writes and
\texttt{quorum} reads on the same data, thus violating strong consistency for other clients. Another
problem is when a client issues \texttt{all} reads and writes, which is completely
unnecessary for enforcing any high-level consistency policies. Similar issues
occur in relational database systems, where it cannot be guaranteed that a
sequence of transactions running at different isolation levels preserves the intended
invariants~\cite{gray1992transaction}. 

% TODO: above is a bit of a jumble. Need to define what is meant by
% consistency. Cassandra examples are a bit unclear.

% TODO the solution is not only for safety, but also for expressiveness. 
The core cause of such problems is that current storage systems allow arbitrary
interactions among low-level operations executing at different consistency levels. To
tackle this problem, we propose a data-centric, declarative programming model, where
consistency requirements are declared for data regions over which a correctness
invariant must hold, and the low-level consistency settings for operations interacting with
a region are assigned or restricted accordingly. In the rest of this paper, we describe our
model and motivate its usefulness in addressing the above issues by showing
an example in a prototype language implementing this model.

\todo{
The rest of this paper is orgranized as follows. Section 2 describes \dots
[if there's room! the paper is so short maybe it makes no sense here]}

\section{Abstract System Model}
Our system model allows developers to split the domain of stored objects into a
collection of named, disjoint
\emph{regions}. Each
region contains objects that are related by a possibly unstated,
application-specific correctness invariant. The
developer annotates each region with a consistency level \texttt{c} $\in$
\texttt{ConsistencyClass}, where \texttt{ConsistencyClass} is a partial order
of consistency definitions based on strength, as defined
in~\cite{sivaramakrishnan2015declarative}. The system assumes that each
set is annotated with a consistency level capable of
enforcing the implied invariant. To interact with regions, the developer
declares \emph{actions}, sequences of atomic
read/write operations that perform a single logical task.
Operations in the same action may interact with multiple sets. An action succeeds if and only if
all contained operations succeed. A consistency policy of a region applies
\emph{only} to all operations within actions interacting with it, as opposed to a complete
history of actions. \todo{huh?} Hence it is important to
correctly group related operations into a single action.
% What are the implications of this??
The visibility
of each write within an action, for both failed and successful
actions is determined by the consistency policy of the region to which it writes.
\todo{I think this is unnecessary... This is not needed for correctness, but
it might be justifiable.}
It is not possible to customize the consistency of any write operation, justified
by the assumption that each data region is annotated with the weakest
consistency set strong enough to enforce an invariant; selecting a stronger
consistency policy is not only pointless, but also can worsen
availability or performance properties of the application. On the other hand,
selecting a weaker consistency policy makes it impossible to prove that
application invariants will appear satisfied for all concurrent operations.
While these assumptions are indeed conservative, they ensure that no accidental
mis-specification of consistency policies on operations will lead to errors. In this way, the
data-centric policies act as data guards, because they force all writes to go
though the needed consistency protocol. Furthermore, preventing writes at higher
consistency levels ensures that the desired availability and partition tolerance
properties are clear and maintained. 

Similarly, the consistency of fetched data is determined by the
policy of the region from which it was read. However, unlike writes, it is possible to
customize the consistency level for reads. It is still not allowed to request reading
at a consistency level stronger than that of the region containing the item
being read, again because it cannot be guaranteed given our restrictions on
write consistency levels; however, it is possible to
request reading at a lower consistency level.
In this case, however, we do not allow
upward writes of data derived from low
consistency reads to regions with a higher consistency level.
% After a read, the local memory
% region to which the data has been read is tagged with the consistency policy of
% the source region.
An implementation may choose to
allow explicit promotion of low-consistency data to be written into higher
consistency regions to increase the flexibility of the model.

%nesting of sets
%weaker containing stronger = ok
%stronger containing weaker = not ok
%merging of sets
%
%I need to specify the interface a store needs to support to be a suitable
%backend for our system, or specify the guarantees of the store.
%could just mention quelea

% TODO: the above is _way_ to vague. I don't know what our contribution is.
% I would still extend the model of [15]
% Something like:
% We have the ordering predicates on actions ww, wr, rw:
% ww(a,b): a writes x (version i), then b writes x (version i+1 [or j > i])
% wr(a,b): a writes x (version i), then b reads x (version i)
% rw(a,b): a reads x (version i), then b writes x (version i+1 [or j > i])
% serializability: ww, wr, rw are acyclic
% eventual consistency and MAV: as in [15]
% with vis = (ww U wr U rw)*  [I think vis is transitive; if not, remove the *]
% extend with causal consistency
% extend with transactions
% extend with sameregion predicates (cf. sameobject)

\section{Sample Implementation and Example}
We now describe a proof of concept implementation, showing only
relevant key points. The objects in our system are integers or strings, represented as replicated registers with LWW
semantics~\cite{burckhardt2014replicated}. At any point in time, different
versions of each entity may
co-exist, either on different replicas or in local client buffers. Each version
is assigned a timestamp as well as other metadata that might be needed to
enforce a consistency property. We define a partial order of three consistency properties, eventual
consistency \texttt{ec} $\prec$ monotonic atomic view \texttt{mav} $\prec$
serializability \texttt{sc}\footnote{The programming language Quelea~\cite{sivaramakrishnan2015declarative} provides similar
  consistency levels, as well as a proof of order validity}. All
objects in our implementation are at least eventually consistent by default, which means it
is possible to freely observe any versions (without monotonicity guarantee).
However, these versions will eventually converge. \texttt{mav}~\cite{bailis2013highly} and \texttt{sc}
can be used as region annotations, and their guarantees apply on the action
level. A region annotated \texttt{mav} guarantees that all contained objects
are observed at the same revision, and revisions are updated monotonically in
terms of versions \todo{revision? define}. The annotation \texttt{sc} on a
region guarantees that the latest version of all
contained objects is observed and manipulated at all times. 

The following listing shows a simple schema with annotated regions and actions:
%\begin{figure}[t!] 
\begin{lstlisting}[basicstyle=\small]
table Customer {
   @sc username: string
   name: string 
   country: string
   @sc region address: 
     (city: string, street: string)
     
   
  @mav region paymentInfo 
     (paymentMethod: string, paymentInfo: string)

}
entity mysteryProduct {
  price: int
  @sc amount: int
}
table opLog {
  username: string
  country: string
  date: string
  amount: int
  orderSucceeded: int
}
action orderMysteryProduct(customer: Customer,
  amount: string) {
  val prevAmount = mysteryProduct.amount
  if(prevAmount - amount >= 0 )
    mysteryProduct.write(prevAmount - amount)
    submitOrder(
     amount, 
     address(customer.city, customer.street), 
     mysteryProduct.price * amount )
     opLog.insert(customer.username, 
        customer.country, date.now(), 1)
  else 
    ERROR
    opLog.insert(customer.username, 
        customer.country, date.now(),, 0)
}
\end{lstlisting}
%\end{figure}

In this example, 
the developer 
decided that \texttt{username}s must be unique, hence serializability is needed
when allocating them. It is still possible to read usernames at lower
consistency levels if needed (e.g., if we only need to display the name). \texttt{name} and \texttt{country} are not very important, even if older
data is shown or intermediate data is written, the customer can still correct
the data and eventually all replicas of these information will converge.
\texttt{address} is strongly consistency in order to avoid shipping to an old address.
payment information is \texttt{mav}, to avoid showing a mixed state of payment info, this still
imposes the problem of using an older payment method when charging the customer,
however, the developer is willing to accept this anomaly for the sake of
availability. \texttt{amount} is \texttt{sc} to ensure that all operations
observe the latest version and avoid selling non-existent items. While it is possible to request a read at a lower
level, our model ensures that we cannot write back an amount based on such reads. For brevity,
we show only one action \texttt{orderMysteryProduct}, which is particularly interesting because it does
not have a single isolation level as is the case in regular transactions.
However, we can see the flexibility and safety this approach gives the
developer. It is possible to read and write at multiple consistency levels
seamlessly, and it is correct according to consistency region specifications.
Actions in our system can be compared to generic functions interacting
with different data stores that enforce varying consistency guarantees.
\texttt{orderMysteryProduct}  can be seen as a function that
reads and updates \texttt{amount} stored in a relational database
transactionally (e.g. ong read and write locks are acquired on amount until
action success is signaled), and also stores operation logs in an eventually consistent,
key-value store. Actions provide the additional property of signaling success
of failure, and developers have to handle them appropriately. For brevity, we omit failure handling details here.

\section{Conclusions, Current and  Future Work}
We have presented here a model that supports only disjoint regions. However, supporting
nesting and merging of regions in the spirit of~\cite{dolby2012data} is necessary to allow expressive and efficient 
implementations. Another feature we are working to support is allowing actions to temporarily merge regions, which
is needed to avoid higher-level data races~\cite{artho2003high}. We are
currently developing a programming language and runtime with support for this extended model
on top of ~\cite{burckhardt2012cloud}. Furthermore, there are systems
that allow declaring invariants directly on data~\cite{bailis2015feral}, instead of 
consistency levels enforcing them as we do. We plan to survey commonly used
invariants in data-backed applications to support declaring them directly, which
is easier and more readable.
%\appendix

%\acks

%cknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\bibliography{../NosheenZazaBib}


\end{document}
